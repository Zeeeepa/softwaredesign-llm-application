{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-20T05:42:44.788557Z",
     "start_time": "2024-02-20T05:42:35.016051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成AIはマーケティング、事業開発、若者文化の各分野で革新をもたらす可能性があります。効率的なコンテンツ作成やターゲティング、製品開発の効率化、若者の創造的な表現の支援など、幅広い活用が期待されます。ただし、データ品質や倫理的観点を考慮しながら慎重に活用する必要があります。生成AIの持つ多様な可能性を探求し、適切に活用することが重要です。\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# gpt-3.5-turbo-0125を使用\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# 三人の専門家の見解を聞くプロンプトを設定\n",
    "expert1_prompt = ChatPromptTemplate.from_template(\"「{topic}」について、マーケティングの専門家による見解を教えて下さい。\")\n",
    "expert2_prompt = ChatPromptTemplate.from_template(\"「{topic}」について、事業開発の専門家による見解を教えて下さい。\")\n",
    "expert3_prompt = ChatPromptTemplate.from_template(\"「{topic}」について、若者文化の専門家による見解を教えて下さい。\")\n",
    "\n",
    "# 各専門家の意見を並列に収集するRunnableParallelを作成\n",
    "expert_opinions = RunnableParallel(\n",
    "    expert1=expert1_prompt | model,\n",
    "    expert2=expert2_prompt | model,\n",
    "    expert3=expert3_prompt | model,\n",
    "    topic=itemgetter(\"topic\")\n",
    ")\n",
    "\n",
    "# 最終的な見解を求めるプロンプトの設定\n",
    "final_judgement_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "マーケティングの専門家の見解: {expert1}\n",
    "事業開発の専門家の見解: {expert2}\n",
    "若者文化の専門家の見解: {expert3}\n",
    "これらの意見を踏まえた上で、{topic}に関する最終的な見解を200字以内で述べて下さい。\n",
    "\"\"\")\n",
    "\n",
    "# 最終的な見解を求めるチェインを作成\n",
    "final_judgement = final_judgement_prompt | model\n",
    "\n",
    "# チェインの実行\n",
    "topic = \"生成AIの可能性\"\n",
    "judgement_chain = expert_opinions | final_judgement | StrOutputParser()\n",
    "result = judgement_chain.invoke({\"topic\": topic})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成AIはマーケティング、事業開発、若者文化の分野で革新的な解決策を提供し、様々な可能性を秘めています。効率的なコンテンツ作成や新規事業創出、若者たちのニーズに合ったコンテンツ提供など、生成AIの活用により、市場での競争力や顧客との関係構築に大きな影響を与えることが期待されます。ただし、技術力や倫理的観点を考慮しながら、専門家の知識と経験を活用して、生成AIの可能性を最大限に引き出す取り組みが重要です。\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# gpt-3.5-turbo-0125を使用\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# 三人の専門家の見解を聞くプロンプトを設定\n",
    "expert1_prompt = ChatPromptTemplate.from_template(\"「{topic}」について、マーケティングの専門家による見解を教えて下さい。\")\n",
    "expert2_prompt = ChatPromptTemplate.from_template(\"「{topic}」について、事業開発の専門家による見解を教えて下さい。\")\n",
    "expert3_prompt = ChatPromptTemplate.from_template(\"「{topic}」について、若者文化の専門家による見解を教えて下さい。\")\n",
    "\n",
    "# 各専門家の意見を並列に収集するRunnableParallelを作成\n",
    "expert_opinions = RunnableParallel(\n",
    "    expert1=expert1_prompt | model,\n",
    "    expert2=expert2_prompt | model,\n",
    "    expert3=expert3_prompt | model,\n",
    "    topic=itemgetter(\"topic\")\n",
    ")\n",
    "\n",
    "# 最終的な見解を求めるプロンプトの設定\n",
    "final_judgement_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "マーケティングの専門家の見解: {expert1}\n",
    "事業開発の専門家の見解: {expert2}\n",
    "若者文化の専門家の見解: {expert3}\n",
    "これらの意見を踏まえた上で、{topic}に関する最終的な見解を200字以内で述べて下さい。\n",
    "\"\"\")\n",
    "\n",
    "# 最終的な見解を求めるチェインを作成\n",
    "final_judgement = final_judgement_prompt | model\n",
    "\n",
    "# チェインの実行\n",
    "topic = \"生成AIの可能性\"\n",
    "judgement_chain = {\"topic\": RunnablePassthrough()} | expert_opinions | final_judgement | StrOutputParser()\n",
    "result = judgement_chain.invoke(topic)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T05:58:33.091202Z",
     "start_time": "2024-02-20T05:58:19.468329Z"
    }
   },
   "id": "dcf5cb6d16d8a220",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "これは テスト用の 文字列です。 余計な空白 と 改行が含まれています。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# 余計な改行や空白を取り除くカスタム関数\n",
    "def clean_input(text):\n",
    "    cleaned_text = ' '.join(text.split())\n",
    "    return cleaned_text\n",
    "\n",
    "# テスト用の入力文字列\n",
    "input_text = \"\"\"\n",
    "    これは    テスト用の\n",
    "    文字列です。      余計な空白    と\n",
    "    改行が含まれています。\n",
    "\"\"\"\n",
    "\n",
    "result = RunnableLambda(clean_input).invoke(input_text)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T07:34:35.180729Z",
     "start_time": "2024-02-20T07:34:35.177323Z"
    }
   },
   "id": "f31e557c96505f3e",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一言で説明: 三毛模様の猫\n",
      "\n",
      "詳しい解説: 「三毛猫」とは、毛の色が白、黒、茶色の3色で模様が入った猫のことを指します。この模様は、遺伝子の影響によって生まれるものであり、日本では縁起の良い猫として親しまれています。三毛猫は性格も賢く、人懐っこいことが多いと言われています。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "与えられた文字列を解説してください。一言で説明した後、改行を入れてから詳しい解説をしてください。\n",
    "与えられた文字列: {text}\n",
    "\"\"\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "runnable = (\n",
    "    {\"text\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ストップシーケンスを設定しない場合\n",
    "text = \"三毛猫\"\n",
    "result = runnable.invoke(text)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T08:45:29.763186Z",
     "start_time": "2024-02-20T08:45:26.917006Z"
    }
   },
   "id": "bc1348ede1db039d",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一言で説明: 三毛模様の猫\n"
     ]
    }
   ],
   "source": [
    "runnable = (\n",
    "    {\"text\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | model.bind(stop=[\"\\n\\n\"])\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ストップシーケンスを設定した場合\n",
    "text = \"三毛猫\"\n",
    "result = runnable.invoke(text)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T08:45:31.639125Z",
     "start_time": "2024-02-20T08:45:30.826574Z"
    }
   },
   "id": "c956fbaa27ef81c1",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「インフルエンサーマーケティング」という手法が注目されています。\n",
      "ただいま対応できる専門家がおりません。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableBranch\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "ROLES = {\n",
    "    \"marketing\": \"マーケティングの専門家\",\n",
    "    \"technology\": \"技術の専門家\",\n",
    "    \"finance\": \"財務の専門家\"\n",
    "}\n",
    "\n",
    "# 入力内容を分類するプロンプト\n",
    "classification_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "与えられた入力内容を以下のどれか一つに分類してください。\n",
    "marketing, technology, finance, other\n",
    "入力内容: {text}\n",
    "\"\"\")\n",
    "\n",
    "# 専門家として見解を述べるプロンプト\n",
    "consultant_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "あなたは{role}です。以下の入力内容に関する見解を述べてください。\n",
    "入力内容: {text}\n",
    "\"\"\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "consultant_chain = {\n",
    "    \"role\": lambda x: ROLES[x[\"role\"]],\n",
    "    \"text\": RunnablePassthrough()\n",
    "} | consultant_prompt | model | StrOutputParser()\n",
    "\n",
    "chain = {\n",
    "    \"text\": RunnablePassthrough(),\n",
    "    \"role\": {\"text\": RunnablePassthrough()} | classification_prompt | model | StrOutputParser()\n",
    " } | RunnableBranch(\n",
    "    (lambda x: x[\"role\"] in ROLES.keys(), consultant_chain),\n",
    "    lambda x: \"ただいま対応できる専門家がおりません。\"\n",
    ")\n",
    "\n",
    "result = chain.invoke(\"最新のマーケティング手法を20字以内で教えて下さい。\")\n",
    "print(result)\n",
    "\n",
    "result = chain.invoke(\"枝豆の妖精について教えて下さい。\")\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T09:21:28.922642Z",
     "start_time": "2024-02-20T09:21:26.446343Z"
    }
   },
   "id": "9eca07b186ef43c6",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpt-3.5': 'Unknown', 'gpt-4': 'GPT-4'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import ConfigurableField, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\").configurable_fields(\n",
    "    model_name=ConfigurableField(id=\"openai_model\", name=\"OpenAI Model\")\n",
    ")\n",
    "chain = model | StrOutputParser()\n",
    "query = \"What is the model number of your version currently in use? Answer in one word.\"\n",
    "result = RunnableParallel({\n",
    "    \"gpt-3.5\": chain,\n",
    "    \"gpt-4\": chain.with_config(configurable={\"openai_model\": \"gpt-4-turbo-preview\"})\n",
    "}).invoke(query)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T09:54:29.602571Z",
     "start_time": "2024-02-20T09:54:26.051117Z"
    }
   },
   "id": "e4a69c6cd31c54e2",
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpt-3.5': 'N/A', 'gpt-4': 'GPT-4'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import ConfigurableField, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI().configurable_alternatives(\n",
    "    ConfigurableField(id=\"openai_model\"),\n",
    "    default_key=\"default\",\n",
    "    gpt3_5=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"),\n",
    "    gpt4=ChatOpenAI(temperature=0, model_name=\"gpt-4-turbo-preview\")\n",
    ")\n",
    "chain = model | StrOutputParser()\n",
    "query = \"What is the model number of your version currently in use? Answer in one word.\"\n",
    "result = RunnableParallel({\n",
    "    \"gpt-3.5\": chain,\n",
    "    \"gpt-4\": chain.with_config(configurable={\"openai_model\": \"gpt4\"})\n",
    "}).invoke(query)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T09:57:53.068218Z",
     "start_time": "2024-02-20T09:57:51.665351Z"
    }
   },
   "id": "14579ea8737c623e",
   "execution_count": 97
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提案された対応策: 顧客に対して、まずは電話やメールでインストール方法を詳細に説明し、必要な資料やリソースを提供することが重要です。その後、もし必要があればリモートサポートを提供するか、訪問サポートを提案することが適切でしょう。最終的には、わかりやすいガイドや動画を提供して、顧客が自分でインストール作業を進める手助けをすることが効果的です。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "@chain\n",
    "def analyze_inquiry_and_suggest_actions(inquiry):\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "    \n",
    "    # 顧客の問い合わせを分析\n",
    "    inquiry_analysis_prompt = ChatPromptTemplate.from_template(\n",
    "        \"以下の顧客からの問い合わせに対する分析を提供してください: {inquiry}\"\n",
    "    )\n",
    "    analysis = (inquiry_analysis_prompt | model | StrOutputParser()).invoke({\"inquiry\": inquiry})\n",
    "    \n",
    "    # 分析結果に基づいて対応策を提案\n",
    "    action_suggestion_prompt = ChatPromptTemplate.from_template(\n",
    "        \"分析: {analysis}\"\n",
    "        \"上記の分析に基づいて、次のステップとして推奨される対応策は何ですか？\"\n",
    "    )\n",
    "    suggested_actions = (action_suggestion_prompt | model | StrOutputParser()).invoke({\"analysis\": analysis})\n",
    "    \n",
    "    return suggested_actions\n",
    "\n",
    "inquiry = \"製品のインストール方法がわかりません。\"\n",
    "suggested_actions = analyze_inquiry_and_suggest_actions.invoke(inquiry)\n",
    "print(f\"提案された対応策: {suggested_actions}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T10:11:17.888236Z",
     "start_time": "2024-02-20T10:11:07.281156Z"
    }
   },
   "id": "13919420f4a4d0cf",
   "execution_count": 106
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+  \n",
      "| analyze_inquiry_and_suggest_actions_input |  \n",
      "+-------------------------------------------+  \n",
      "                       *                       \n",
      "                       *                       \n",
      "                       *                       \n",
      "  +---------------------------------------+    \n",
      "  | Lambda(analyze_inquiry_and_suggest... |    \n",
      "  +---------------------------------------+    \n",
      "                       *                       \n",
      "                       *                       \n",
      "                       *                       \n",
      "+--------------------------------------------+ \n",
      "| analyze_inquiry_and_suggest_actions_output | \n",
      "+--------------------------------------------+ \n"
     ]
    }
   ],
   "source": [
    "analyze_inquiry_and_suggest_actions.get_graph().print_ascii()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T10:08:15.863181Z",
     "start_time": "2024-02-20T10:08:15.857017Z"
    }
   },
   "id": "ff5f7354aed98c19",
   "execution_count": 105
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_1: おいしい朝ご飯ですね。\n",
      "session_1: 納豆を食べました。\n",
      "session_2: わかりません。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# モデルとプロンプトの設定\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"あなたは20字以内で答えるAIです\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# 会話履歴をセッションID毎に管理する関数\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 会話履歴機能を備えたRunnableの構成\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    (prompt | model | StrOutputParser()),\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# チャット履歴を利用してRunnableを実行\n",
    "session_id = \"session_1\"\n",
    "response = with_message_history.invoke(\n",
    "    {\"input\": \"朝ご飯に納豆を食べたよ\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "print(f\"{session_id}:\", response)\n",
    "\n",
    "# 同じセッションIDで再度実行し、会話履歴を利用\n",
    "response = with_message_history.invoke(\n",
    "    {\"input\": \"朝何食べたっけ？\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "print(f\"{session_id}:\", response)\n",
    "\n",
    "# 異なるセッションIDで実行\n",
    "session_id = \"session_2\"\n",
    "response = with_message_history.invoke(\n",
    "    {\"input\": \"朝何食べたっけ？\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "print(f\"{session_id}:\", response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T10:28:08.733847Z",
     "start_time": "2024-02-20T10:28:06.565126Z"
    }
   },
   "id": "2b307a45b5a6dcf7",
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "94bf5a5e2c510c05"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
