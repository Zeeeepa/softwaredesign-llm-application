{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain v1.0 create_agent 解説ノートブック\n",
    "\n",
    "このノートブックは、LangChain v1.0で導入された`create_agent`とミドルウェアの仕組みを実際に動かしながら理解するためのものです。\n",
    "\n",
    "**動作環境**\n",
    "- Python 3.12\n",
    "- langchain 1.1.3\n",
    "- langchain-anthropic 1.2.0\n",
    "- langgraph 1.0.4\n",
    "\n",
    "**注意**: このノートブックを実行するとAnthropic APIが呼び出され、料金が発生します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクション1: 環境セットアップ\n",
    "\n",
    "まず、必要なライブラリをインポートし、環境変数を読み込みます。\n",
    "`.env`ファイルに`ANTHROPIC_API_KEY`が設定されている必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 基本的なインポート\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "\n",
    "print(\"環境セットアップ完了!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクション2: create_agentの基本\n",
    "\n",
    "`create_agent`は、LangChain v1.0でAIエージェントを作成するための中心的なAPIです。\n",
    "最もシンプルな形でエージェントを作成してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最もシンプルなエージェント (ツールなし)\n",
    "simple_agent = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
    "    tools=[],  # ツールなし\n",
    "    system_prompt=\"あなたは親切なアシスタントです。\"\n",
    ")\n",
    "\n",
    "# エージェントを実行\n",
    "result = simple_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"こんにちは！簡単に自己紹介してください。\"}]\n",
    "})\n",
    "\n",
    "# 結果を確認\n",
    "print(\"=== 結果の構造 ===\")\n",
    "print(f\"キー: {result.keys()}\")\n",
    "print()\n",
    "print(\"=== 最後のメッセージ ===\")\n",
    "last_message = result[\"messages\"][-1]\n",
    "print(f\"タイプ: {type(last_message).__name__}\")\n",
    "print(f\"内容: {last_message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクション3: toolsパラメータの指定方法\n",
    "\n",
    "`@tool`デコレータを使用してツールを定義し、エージェントに渡します。\n",
    "型ヒントとdocstringは必須で、これがモデルに渡されるツールの説明になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"Web検索を実行する\"\"\"\n",
    "    return f\"「{query}」の検索結果: Pythonは汎用プログラミング言語です。\"\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"指定した都市の天気を取得する\"\"\"\n",
    "    return f\"{city}の天気: 晴れ、気温25度\"\n",
    "\n",
    "# ツール付きエージェントを作成\n",
    "agent_with_tools = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
    "    tools=[search_web, get_weather],\n",
    "    system_prompt=\"あなたは検索と天気情報を提供するアシスタントです。\"\n",
    ")\n",
    "\n",
    "# ツールが呼ばれる質問をする\n",
    "result = agent_with_tools.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"東京の天気を教えてください\"}]\n",
    "})\n",
    "\n",
    "# 結果を確認\n",
    "print(\"=== メッセージ履歴 ===\")\n",
    "for i, msg in enumerate(result[\"messages\"]):\n",
    "    msg_type = type(msg).__name__\n",
    "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "        print(f\"{i}: [{msg_type}] ツール呼び出し: {[tc['name'] for tc in msg.tool_calls]}\")\n",
    "    elif hasattr(msg, 'name'):\n",
    "        print(f\"{i}: [{msg_type}] {msg.name}: {msg.content[:50]}...\")\n",
    "    else:\n",
    "        content = msg.content if isinstance(msg.content, str) else str(msg.content)\n",
    "        print(f\"{i}: [{msg_type}] {content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクション4: modelパラメータの指定方法\n",
    "\n",
    "モデルは文字列またはモデルインスタンスで指定できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_anthropic import ChatAnthropic\n\n# 方法1: 文字列で指定\nagent1 = create_agent(\n    model=\"anthropic:claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n)\n\n# 方法1の実行テスト\nresult1 = agent1.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"京都の天気は?\"}]\n})\nprint(\"=== 方法1: 文字列で指定 ===\")\nprint(f\"応答: {result1['messages'][-1].content}\")\n\n# 方法2: ChatAnthropicインスタンスで指定 (詳細設定が可能)\nmodel = ChatAnthropic(\n    model=\"claude-sonnet-4-5-20250929\",\n    temperature=0,\n    max_tokens=1024,\n    timeout=30,\n)\n\nagent2 = create_agent(\n    model=model,\n    tools=[get_weather],\n)\n\n# 方法2の実行テスト\nresult2 = agent2.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"大阪の天気は?\"}]\n})\nprint(\"\\n=== 方法2: ChatAnthropicインスタンスで指定 ===\")\nprint(f\"応答: {result2['messages'][-1].content}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクション5: system_promptパラメータ\n",
    "\n",
    "システムプロンプトは文字列または`SystemMessage`オブジェクトで指定できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# 方法1: 文字列で指定\n",
    "agent_str_prompt = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
    "    tools=[],\n",
    "    system_prompt=\"あなたは関西弁で話すアシスタントです。\",\n",
    ")\n",
    "\n",
    "result = agent_str_prompt.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"今日の調子はどう?\"}]\n",
    "})\n",
    "print(\"=== 文字列でのシステムプロンプト ===\")\n",
    "print(result[\"messages\"][-1].content)\n",
    "\n",
    "# 方法2: SystemMessageで指定 (プロンプトキャッシュなどの高度な機能が使える)\n",
    "# ※ cache_controlはAnthropicの機能で、大量のコンテキストをキャッシュできる\n",
    "agent_sys_msg = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
    "    tools=[],\n",
    "    system_prompt=SystemMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": \"あなたは敬語で話すアシスタントです。\"},\n",
    "            # 大量のコンテキスト情報がある場合、cache_controlを使うと効率的\n",
    "            # {\"type\": \"text\", \"text\": \"<大量のコンテキスト>\", \"cache_control\": {\"type\": \"ephemeral\"}}\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "result = agent_sys_msg.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"今日の調子はどう?\"}]\n",
    "})\n",
    "print(\"\\n=== SystemMessageでのシステムプロンプト ===\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクション6: response_formatパラメータ\n",
    "\n",
    "`response_format`を使用すると、AIエージェントの出力を構造化されたフォーマットで取得できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    \"\"\"連絡先情報\"\"\"\n",
    "    name: str = Field(description=\"名前\")\n",
    "    email: str = Field(description=\"メールアドレス\")\n",
    "    phone: str = Field(description=\"電話番号\")\n",
    "\n",
    "# 構造化出力を指定したエージェント\n",
    "structured_agent = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
    "    tools=[],\n",
    "    response_format=ToolStrategy(ContactInfo),\n",
    ")\n",
    "\n",
    "result = structured_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"田中太郎、tanaka@example.com、03-1234-5678\"}]\n",
    "})\n",
    "\n",
    "print(\"=== 構造化された出力 ===\")\n",
    "print(f\"structured_response: {result.get('structured_response')}\")\n",
    "print()\n",
    "if result.get('structured_response'):\n",
    "    contact = result['structured_response']\n",
    "    print(f\"名前: {contact.name}\")\n",
    "    print(f\"メール: {contact.email}\")\n",
    "    print(f\"電話: {contact.phone}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクション7: 短期記憶 (checkpointer)\n",
    "\n",
    "`checkpointer`を使用すると、会話履歴を保持して複数ターンの会話が可能になります。\n",
    "`thread_id`ごとに会話が分離されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# checkpointer付きエージェント\n",
    "memory_agent = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
    "    tools=[],\n",
    "    system_prompt=\"あなたは親切なアシスタントです。\",\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "# 会話1: 最初のメッセージ\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation-001\"}}\n",
    "\n",
    "result1 = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"私の名前は田中です。覚えておいてください。\"}]},\n",
    "    config=config,\n",
    ")\n",
    "print(\"=== ターン1 ===\")\n",
    "print(result1[\"messages\"][-1].content)\n",
    "\n",
    "# 会話2: 同じthread_idで続ける (前の会話を覚えている)\n",
    "result2 = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"私の名前を覚えていますか?\"}]},\n",
    "    config=config,\n",
    ")\n",
    "print(\"\\n=== ターン2 (同じthread_id) ===\")\n",
    "print(result2[\"messages\"][-1].content)\n",
    "\n",
    "# 会話3: 異なるthread_idで実行 (新しい会話)\n",
    "config_new = {\"configurable\": {\"thread_id\": \"conversation-002\"}}\n",
    "result3 = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"私の名前を覚えていますか?\"}]},\n",
    "    config=config_new,\n",
    ")\n",
    "print(\"\\n=== ターン3 (別のthread_id) ===\")\n",
    "print(result3[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクション8: ToolRuntimeの活用\n",
    "\n",
    "`ToolRuntime`を使うと、ツール内から状態やコンテキストにアクセスできます。\n",
    "`runtime`パラメータはモデルには見えず、実行時に自動で注入されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import ToolRuntime\n",
    "from typing import TypedDict\n",
    "\n",
    "# コンテキストスキーマを定義\n",
    "class UserContext(TypedDict):\n",
    "    user_name: str\n",
    "    user_id: str\n",
    "\n",
    "@tool\n",
    "def get_user_info(runtime: ToolRuntime) -> str:\n",
    "    \"\"\"現在のユーザー情報を取得する\"\"\"\n",
    "    user_name = runtime.context.get(\"user_name\", \"不明\")\n",
    "    user_id = runtime.context.get(\"user_id\", \"不明\")\n",
    "    return f\"ユーザー名: {user_name}, ID: {user_id}\"\n",
    "\n",
    "@tool\n",
    "def count_messages(runtime: ToolRuntime) -> str:\n",
    "    \"\"\"会話のメッセージ数を取得する\"\"\"\n",
    "    messages = runtime.state.get(\"messages\", [])\n",
    "    return f\"現在のメッセージ数: {len(messages)}件\"\n",
    "\n",
    "# context_schema付きエージェント\n",
    "context_agent = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
    "    tools=[get_user_info, count_messages],\n",
    "    context_schema=UserContext,\n",
    "    system_prompt=\"ツールを使ってユーザー情報を取得できます。\",\n",
    ")\n",
    "\n",
    "# コンテキストを渡して実行\n",
    "result = context_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"私のユーザー情報を教えてください\"}]},\n",
    "    context={\"user_name\": \"田中太郎\", \"user_id\": \"user_123\"},\n",
    ")\n",
    "\n",
    "print(\"=== ToolRuntimeでコンテキストにアクセス ===\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクション9: 長期記憶 (store)\n",
    "\n",
    "`store`を使用すると、セッションを超えて永続化される情報を管理できます。\n",
    "短期記憶(checkpointer)との違いは、スコープがセッションを超えて永続することです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# ストアを作成\n",
    "store = InMemoryStore()\n",
    "\n",
    "# 事前にデータを保存\n",
    "store.put((\"users\",), \"user_123\", {\"name\": \"田中太郎\", \"preference\": \"Python\"})\n",
    "\n",
    "@tool\n",
    "def get_stored_user_info(user_id: str, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"ストアからユーザー情報を取得する\"\"\"\n",
    "    info = runtime.store.get((\"users\",), user_id)\n",
    "    if info:\n",
    "        return f\"ユーザー情報: {info.value}\"\n",
    "    return f\"ユーザー {user_id} は見つかりませんでした\"\n",
    "\n",
    "@tool\n",
    "def save_user_preference(user_id: str, preference: str, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"ユーザーの好みをストアに保存する\"\"\"\n",
    "    existing = runtime.store.get((\"users\",), user_id)\n",
    "    data = existing.value if existing else {}\n",
    "    data[\"preference\"] = preference\n",
    "    runtime.store.put((\"users\",), user_id, data)\n",
    "    return f\"ユーザー {user_id} の好みを '{preference}' に更新しました\"\n",
    "\n",
    "# store付きエージェント\n",
    "store_agent = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
    "    tools=[get_stored_user_info, save_user_preference],\n",
    "    store=store,\n",
    "    system_prompt=\"ユーザー情報の取得と保存ができます。\",\n",
    ")\n",
    "\n",
    "# ユーザー情報を取得\n",
    "result = store_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"user_123の情報を教えてください\"}]\n",
    "})\n",
    "print(\"=== ストアからの取得 ===\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクション10: state_schemaとcontext_schema\n",
    "\n",
    "- `state_schema`: 実行中に変化する可能性のある情報 (AgentStateを継承)\n",
    "- `context_schema`: 実行開始時に設定され、変化しない情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentState\n",
    "\n",
    "# カスタムステートを定義 (AgentStateを継承)\n",
    "class CustomState(AgentState):\n",
    "    user_preferences: dict\n",
    "    session_count: int\n",
    "\n",
    "# カスタムコンテキストを定義\n",
    "class AppContext(TypedDict):\n",
    "    user_id: str\n",
    "    environment: str  # \"development\" or \"production\"\n",
    "\n",
    "@tool\n",
    "def show_state_info(runtime: ToolRuntime) -> str:\n",
    "    \"\"\"現在の状態とコンテキスト情報を表示する\"\"\"\n",
    "    prefs = runtime.state.get(\"user_preferences\", {})\n",
    "    count = runtime.state.get(\"session_count\", 0)\n",
    "    user_id = runtime.context.get(\"user_id\", \"不明\")\n",
    "    env = runtime.context.get(\"environment\", \"不明\")\n",
    "    return f\"状態 - 設定: {prefs}, セッション数: {count} / コンテキスト - ユーザーID: {user_id}, 環境: {env}\"\n",
    "\n",
    "# state_schemaとcontext_schema両方を指定\n",
    "custom_agent = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
    "    tools=[show_state_info],\n",
    "    state_schema=CustomState,\n",
    "    context_schema=AppContext,\n",
    "    system_prompt=\"状態とコンテキストの情報を表示できます。\",\n",
    ")\n",
    "\n",
    "# 初期状態とコンテキストを渡して実行\n",
    "result = custom_agent.invoke(\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"現在の状態情報を教えてください\"}],\n",
    "        \"user_preferences\": {\"theme\": \"dark\", \"language\": \"ja\"},\n",
    "        \"session_count\": 5,\n",
    "    },\n",
    "    context={\"user_id\": \"user_456\", \"environment\": \"production\"},\n",
    ")\n",
    "\n",
    "print(\"=== state_schemaとcontext_schemaの確認 ===\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクション11: ミドルウェア - デコレータベース\n",
    "\n",
    "ミドルウェアは、AIエージェントの実行フローに介入して動作を制御する仕組みです。\n",
    "デコレータを使用して簡単に定義できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import before_model, after_model, dynamic_prompt, ModelRequest\n",
    "\n",
    "@before_model\n",
    "def log_before_model(state, runtime):\n",
    "    \"\"\"モデル呼び出し前にログを出力\"\"\"\n",
    "    print(f\"[BEFORE] モデル呼び出し開始: {len(state['messages'])}件のメッセージ\")\n",
    "    return None  # 状態を変更しない場合はNoneを返す\n",
    "\n",
    "@after_model\n",
    "def log_after_model(state, runtime):\n",
    "    \"\"\"モデル呼び出し後にログを出力\"\"\"\n",
    "    print(f\"[AFTER] モデル呼び出し完了\")\n",
    "    return None\n",
    "\n",
    "# デコレータベースのミドルウェアを使用\n",
    "logged_agent = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
    "    tools=[],\n",
    "    middleware=[log_before_model, log_after_model],\n",
    "    system_prompt=\"あなたはアシスタントです。\",\n",
    ")\n",
    "\n",
    "print(\"=== ミドルウェアのログを確認 ===\")\n",
    "result = logged_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"こんにちは\"}]\n",
    "})\n",
    "print(f\"応答: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dynamic_promptを使用した動的プロンプト生成\n",
    "@dynamic_prompt\n",
    "def role_based_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"ユーザーの役割に応じてプロンプトを変更\"\"\"\n",
    "    user_role = request.runtime.context.get(\"user_role\", \"user\")\n",
    "    if user_role == \"admin\":\n",
    "        return \"あなたは管理者向けアシスタントです。詳細な技術情報を提供してください。\"\n",
    "    elif user_role == \"expert\":\n",
    "        return \"あなたは専門家向けアシスタントです。専門用語を使って説明してください。\"\n",
    "    return \"あなたは一般ユーザー向けアシスタントです。分かりやすく説明してください。\"\n",
    "\n",
    "class RoleContext(TypedDict):\n",
    "    user_role: str\n",
    "\n",
    "dynamic_agent = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
    "    tools=[],\n",
    "    context_schema=RoleContext,\n",
    "    middleware=[role_based_prompt],\n",
    ")\n",
    "\n",
    "# 一般ユーザーとして質問\n",
    "result_user = dynamic_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"APIとは何ですか?\"}]},\n",
    "    context={\"user_role\": \"user\"},\n",
    ")\n",
    "print(\"=== 一般ユーザー向け応答 ===\")\n",
    "print(result_user[\"messages\"][-1].content[:200] + \"...\")\n",
    "\n",
    "# 専門家として質問\n",
    "result_expert = dynamic_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"APIとは何ですか?\"}]},\n",
    "    context={\"user_role\": \"expert\"},\n",
    ")\n",
    "print(\"\\n=== 専門家向け応答 ===\")\n",
    "print(result_expert[\"messages\"][-1].content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクション12: ミドルウェア - クラスベース\n",
    "\n",
    "複数のフックを組み合わせたり、状態を保持する必要がある場合は、\n",
    "`AgentMiddleware`クラスを継承してミドルウェアを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n",
    "from typing import Callable, Any\n",
    "\n",
    "class LoggingMiddleware(AgentMiddleware):\n",
    "    \"\"\"ロギング機能を持つカスタムミドルウェア\"\"\"\n",
    "    \n",
    "    def __init__(self, log_level: str = \"INFO\"):\n",
    "        super().__init__()\n",
    "        self.log_level = log_level\n",
    "        self.call_count = 0\n",
    "    \n",
    "    def before_agent(self, state, runtime) -> dict[str, Any] | None:\n",
    "        print(f\"[{self.log_level}] AIエージェント開始\")\n",
    "        return None\n",
    "    \n",
    "    def wrap_model_call(\n",
    "        self,\n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], ModelResponse]\n",
    "    ) -> ModelResponse:\n",
    "        self.call_count += 1\n",
    "        print(f\"[{self.log_level}] モデル呼び出し #{self.call_count}: {len(request.messages)}件のメッセージ\")\n",
    "        response = handler(request)\n",
    "        print(f\"[{self.log_level}] モデル応答受信\")\n",
    "        return response\n",
    "    \n",
    "    def after_agent(self, state, runtime) -> dict[str, Any] | None:\n",
    "        print(f\"[{self.log_level}] AIエージェント完了 (合計 {self.call_count} 回のモデル呼び出し)\")\n",
    "        return None\n",
    "\n",
    "# クラスベースのミドルウェアを使用\n",
    "class_middleware_agent = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
    "    tools=[get_weather],\n",
    "    middleware=[LoggingMiddleware(log_level=\"DEBUG\")],\n",
    "    system_prompt=\"天気情報を提供するアシスタントです。\",\n",
    ")\n",
    "\n",
    "print(\"=== クラスベースミドルウェアの動作 ===\")\n",
    "result = class_middleware_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"東京の天気を教えて\"}]\n",
    "})\n",
    "print(f\"\\n応答: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクション13: ModelRequestとModelResponse\n",
    "\n",
    "`wrap_model_call`フックでは、`ModelRequest`と`ModelResponse`を操作できます。\n",
    "`request.override()`を使用してリクエストを変更できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import wrap_model_call\n",
    "\n",
    "@wrap_model_call\n",
    "def add_context_to_request(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"リクエストにコンテキスト情報を追加\"\"\"\n",
    "    # 現在時刻を追加情報としてメッセージに追加\n",
    "    from datetime import datetime\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # 元のメッセージを取得\n",
    "    original_messages = list(request.messages)\n",
    "    \n",
    "    # システムメッセージを追加 (最初のユーザーメッセージの前に)\n",
    "    context_msg = {\"role\": \"system\", \"content\": f\"現在時刻: {current_time}\"}\n",
    "    \n",
    "    # メッセージリストを更新\n",
    "    new_messages = [context_msg] + original_messages\n",
    "    \n",
    "    # リクエストを変更して実行\n",
    "    modified_request = request.override(messages=new_messages)\n",
    "    print(f\"[wrap_model_call] コンテキスト追加: {current_time}\")\n",
    "    \n",
    "    return handler(modified_request)\n",
    "\n",
    "# wrap_model_callを使用するエージェント\n",
    "wrap_agent = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
    "    tools=[],\n",
    "    middleware=[add_context_to_request],\n",
    "    system_prompt=\"あなたは時間を意識したアシスタントです。\",\n",
    ")\n",
    "\n",
    "result = wrap_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"今何時ですか?\"}]\n",
    "})\n",
    "print(f\"応答: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動的モデル選択の例\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# 2つのモデルを用意\n",
    "fast_model = ChatAnthropic(model=\"claude-haiku-4-5-20251001\")\n",
    "smart_model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n",
    "\n",
    "@wrap_model_call\n",
    "def select_model_by_complexity(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"メッセージの長さに応じてモデルを切り替え\"\"\"\n",
    "    # 最後のユーザーメッセージを取得\n",
    "    user_messages = [m for m in request.messages if hasattr(m, 'content') and getattr(m, 'type', None) == 'human']\n",
    "    if user_messages:\n",
    "        last_user_msg = user_messages[-1]\n",
    "        content = last_user_msg.content if isinstance(last_user_msg.content, str) else str(last_user_msg.content)\n",
    "        msg_length = len(content)\n",
    "    else:\n",
    "        msg_length = 0\n",
    "    \n",
    "    # メッセージが長い場合は高性能モデルを使用\n",
    "    if msg_length > 100:\n",
    "        print(f\"[動的選択] 長いメッセージ ({msg_length}文字) -> claude-sonnet-4-5\")\n",
    "        modified = request.override(model=smart_model)\n",
    "    else:\n",
    "        print(f\"[動的選択] 短いメッセージ ({msg_length}文字) -> claude-haiku-4-5\")\n",
    "        modified = request.override(model=fast_model)\n",
    "    \n",
    "    return handler(modified)\n",
    "\n",
    "dynamic_model_agent = create_agent(\n",
    "    model=fast_model,  # デフォルトはfast_model\n",
    "    tools=[],\n",
    "    middleware=[select_model_by_complexity],\n",
    "    system_prompt=\"あなたはアシスタントです。\",\n",
    ")\n",
    "\n",
    "# 短いメッセージ\n",
    "print(\"=== 短いメッセージ ===\")\n",
    "result1 = dynamic_model_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"こんにちは\"}]\n",
    "})\n",
    "print(f\"応答: {result1['messages'][-1].content}\\n\")\n",
    "\n",
    "# 長いメッセージ\n",
    "print(\"=== 長いメッセージ ===\")\n",
    "long_message = \"私は最近プログラミングを始めました。特にPythonに興味があります。機械学習やデータサイエンスの分野で活用したいと考えています。初心者におすすめの学習リソースや、効率的な学習方法があれば教えてください。\"\n",
    "result2 = dynamic_model_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": long_message}]\n",
    "})\n",
    "print(f\"応答: {result2['messages'][-1].content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクション14: 標準ミドルウェアの紹介\n",
    "\n",
    "LangChainは、一般的なユースケースに対応するミドルウェアを標準で提供しています。\n",
    "\n",
    "| ミドルウェア | 説明 |\n",
    "|-------------|------|\n",
    "| SummarizationMiddleware | トークン制限に近づいたときに会話履歴を自動で要約する |\n",
    "| HumanInTheLoopMiddleware | ツール呼び出し前に人間の承認を要求する |\n",
    "| ModelCallLimitMiddleware | モデル呼び出し回数を制限し、過剰なコストを防ぐ |\n",
    "| ToolCallLimitMiddleware | ツールの呼び出し回数を制限する |\n",
    "| ModelFallbackMiddleware | メインモデルが失敗した場合に代替モデルに切り替える |\n",
    "| PIIMiddleware | 個人情報（PII）を検出してマスキングまたはブロックする |\n",
    "| TodoListMiddleware | AIエージェントにタスク計画と進捗管理の機能を提供する |\n",
    "| LLMToolSelectorMiddleware | メインモデル呼び出し前に、LLMで関連するツールを選択する |\n",
    "| ToolRetryMiddleware | ツール呼び出しの失敗時に指数バックオフで自動リトライする |\n",
    "| ModelRetryMiddleware | モデル呼び出しの失敗時に指数バックオフで自動リトライする |\n",
    "| LLMToolEmulatorMiddleware | テスト目的でツール実行をLLMでエミュレートする |\n",
    "| ContextEditingMiddleware | 古いツール結果を削除してコンテキストを管理する |\n",
    "| ShellToolMiddleware | AIエージェントにシェルコマンド実行機能を提供する |\n",
    "| FilesystemFileSearchMiddleware | ファイルシステムに対するGlob/Grep検索機能を提供する |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain.agents.middleware import (\n    SummarizationMiddleware,\n    ModelCallLimitMiddleware,\n    ToolRetryMiddleware,\n)\n\n# =====================================\n# 1. SummarizationMiddleware\n# =====================================\nprint(\"=\" * 50)\nprint(\"1. SummarizationMiddleware\")\nprint(\"=\" * 50)\n\nsummarization_agent = create_agent(\n    model=\"anthropic:claude-sonnet-4-5-20250929\",\n    tools=[],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"anthropic:claude-haiku-4-5-20251001\",\n            trigger=(\"tokens\", 4000),\n            keep=(\"messages\", 10),\n        )\n    ],\n    checkpointer=InMemorySaver(),\n)\n\n# 実行: 複数ターンの会話\nconfig = {\"configurable\": {\"thread_id\": \"summarization-test\"}}\nresult = summarization_agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"こんにちは、今日は良い天気ですね\"}]},\n    config=config,\n)\nprint(f\"応答: {result['messages'][-1].content}\")\n\n# =====================================\n# 2. ModelCallLimitMiddleware\n# =====================================\nprint(\"\\n\" + \"=\" * 50)\nprint(\"2. ModelCallLimitMiddleware\")\nprint(\"=\" * 50)\n\nlimited_agent = create_agent(\n    model=\"anthropic:claude-sonnet-4-5-20250929\",\n    tools=[search_web],\n    middleware=[\n        ModelCallLimitMiddleware(\n            thread_limit=10,\n            run_limit=3,\n            exit_behavior=\"end\",\n        )\n    ],\n)\n\nresult = limited_agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Pythonについて教えて\"}]\n})\nprint(f\"応答: {result['messages'][-1].content[:200]}...\")\n\n# =====================================\n# 3. ToolRetryMiddleware\n# =====================================\nprint(\"\\n\" + \"=\" * 50)\nprint(\"3. ToolRetryMiddleware\")\nprint(\"=\" * 50)\n\nimport random\nrandom.seed(42)  # 再現性のためシードを固定\n\n@tool\ndef unreliable_api(query: str) -> str:\n    \"\"\"不安定なAPIを呼び出す (テスト用)\"\"\"\n    if random.random() < 0.3:  # 30%の確率で失敗\n        raise ConnectionError(\"API接続エラー\")\n    return f\"APIの結果: {query}に関する情報です\"\n\nretry_agent = create_agent(\n    model=\"anthropic:claude-sonnet-4-5-20250929\",\n    tools=[unreliable_api],\n    middleware=[\n        ToolRetryMiddleware(\n            max_retries=3,\n            backoff_factor=2.0,\n            initial_delay=0.1,\n        )\n    ],\n)\n\nresult = retry_agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"最新のニュースを教えて\"}]\n})\nprint(f\"応答: {result['messages'][-1].content[:200]}...\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}